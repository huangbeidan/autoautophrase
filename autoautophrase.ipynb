{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/beidan/RASHIP/AutoPhrase/models/DBLP/pattern_score_details_iter1.txt\", newline = '') as games:                                                                                          \n",
    "        game_reader = pd.read_table(games, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/beidan/RASHIP/AutoPhrase/tmp/pattern_score_details_1.txt\", newline = '') as games:                                                                                          \n",
    "#         game_reader = pd.read_table(games, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_reader = game_reader[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_reader.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_reader[game_reader['pattern'].str.contains(\"tricare\")].sort_values('final_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"a\":1}\n",
    "if test:\n",
    "    print(1)\n",
    "else:\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Patricia class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My custom tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, token, score, stopW = False):\n",
    "        self.token = token\n",
    "        self.count = 1\n",
    "        self.qualitylist = [score]\n",
    "        self.isStopword = stopW\n",
    "        \n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class patricia():\n",
    "    def __init__(self):\n",
    "        self._data = {}\n",
    "        self._tokenMap = {}\n",
    "    \n",
    "    def incr(self, word):\n",
    "        tokenMap = self._tokenMap\n",
    "        if word.token in tokenMap:\n",
    "            tokenMap[word.token].count += 1\n",
    "            tokenMap[word.token].qualitylist += word.qualitylist\n",
    "        else:\n",
    "            tokenMap[word.token] = word\n",
    "\n",
    "    def addWord(self, word):\n",
    "        self.incr(word)\n",
    "        \n",
    "        data = self._data\n",
    "        i = 0\n",
    "        while 1:\n",
    "            try:\n",
    "                # try this char by char\n",
    "                node = data[word.token[i:i+1]]\n",
    "            \n",
    "            # do not have the key\n",
    "            except KeyError:\n",
    "                \n",
    "                \n",
    "                # if the dict is not empty\n",
    "                if data:\n",
    "                    # initialize a new branch\n",
    "                    data[word.token[i:i+1]] = [word.token[i+1:],{}]\n",
    "                    \n",
    "                else:\n",
    "                    # if end of word - return\n",
    "                    if word.token[i:i+1] == '':\n",
    "                        return\n",
    "                    else:\n",
    "                        # if it is not the first character\n",
    "                        if i != 0:\n",
    "                            data[''] = ['',{}]\n",
    "                        # put the rest of words in\n",
    "                        data[word.token[i:i+1]] = [word.token[i+1:],{}]\n",
    "                return\n",
    "\n",
    "            # have the key\n",
    "            i += 1\n",
    "            \n",
    "            # if the word starts with the prefix starting at position i\n",
    "            if word.token.startswith(node[0],i):\n",
    "                \n",
    "                \n",
    "                # if the trie always has the prefix tree\n",
    "                #e.g. tricare --> i=1 -> node[0] icare  word[i:] = icare\n",
    "                if len(word.token[i:]) == len(node[0]):\n",
    "                    #print (\"word {}, i {}, node[0] {}\", word, i, node[0])  \n",
    "                     \n",
    "                    # if the trie has right\n",
    "                    if node[1]:\n",
    "                        try:\n",
    "                            node[1]['']\n",
    "                        except KeyError:\n",
    "                            data = node[1]\n",
    "                            data[''] = ['',{}]\n",
    "                    return\n",
    "                # node[0] is the current prefix\n",
    "                else:\n",
    "                    \n",
    "                    # directly advance\n",
    "                    i += len(node[0])\n",
    "                    data = node[1]\n",
    "            \n",
    "            else:\n",
    "                ii = i\n",
    "                j = 0\n",
    "                while ii != len(word.token) and j != len(node[0]) and \\\n",
    "                      word.token[ii:ii+1] == node[0][j:j+1]:\n",
    "                    ii += 1\n",
    "                    j += 1\n",
    "                tmpdata = {}\n",
    "                tmpdata[node[0][j:j+1]] = [node[0][j+1:],node[1]]\n",
    "                tmpdata[word.token[ii:ii+1]] = [word.token[ii+1:],{}]\n",
    "                \n",
    "                #print(\"tempdata, nord[0], word_part: \", str(tmpdata), node[0],word[ii+1:] )\n",
    "                data[word.token[i-1:i]] = [node[0][:j],tmpdata]\n",
    "                return\n",
    "            \n",
    "    def isPrefix(self,word):\n",
    "        data = self._data\n",
    "        i = 0\n",
    "        wordlen = len(word.token)\n",
    "        while 1:\n",
    "            try:\n",
    "                node = data[word.token[i:i+1]]\n",
    "            except KeyError:\n",
    "                return False\n",
    "            i += 1\n",
    "            if word.token.startswith(node[0][:wordlen-i],i):\n",
    "                if wordlen - i > len(node[0]):\n",
    "                    i += len(node[0])\n",
    "                    data = node[1]\n",
    "                else:\n",
    "                    return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    def isWord(self,word):\n",
    "        data = self._data\n",
    "        i = 0\n",
    "        while 1:\n",
    "            try:\n",
    "                node = data[word.token[i:i+1]]\n",
    "            except KeyError:\n",
    "                return False\n",
    "            i += 1\n",
    "            if word.token.startswith(node[0],i):\n",
    "                if len(word.token[i:]) == len(node[0]):\n",
    "                    if node[1]:\n",
    "                        try:\n",
    "                            node[1]['']\n",
    "                        except KeyError:\n",
    "                            return False\n",
    "                    return True\n",
    "                else:\n",
    "                    i += len(node[0])\n",
    "                    data = node[1]\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    def printCount(self):\n",
    "        info = {}\n",
    "        tokenmap = self._tokenMap\n",
    "        for token in tokenmap:\n",
    "            info[token] = tokenmap[token].count\n",
    "        print(info)\n",
    "    \n",
    "    def getVarAll(self):\n",
    "        info = {}\n",
    "        tokenmap = self._tokenMap\n",
    "        for token in tokenmap:\n",
    "            if token not in set(stopwords.words('english')):\n",
    "                ls = tokenmap[token].qualitylist\n",
    "                if (len(ls) < 2):\n",
    "                        info[token] = 0\n",
    "                else:\n",
    "                        info[token] = statistics.variance(ls) * math.log(len(ls))\n",
    "\n",
    "        return (info)\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct a list of Word and rank it by variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initialize\n",
    "dt = defaultdict()\n",
    "xxx = patricia()\n",
    "for index, row in tqdm(game_reader.iterrows()):\n",
    "        pattern = row['pattern']\n",
    "        tokens = pattern.split(' ')\n",
    "        score = row['final_score']\n",
    "        for token in tokens:\n",
    "            xxx.addWord(Word(token, score))\n",
    "\n",
    "## now we save all the data in xxx\n",
    "## add counter to list of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxx.printCount()\n",
    "varsDict = xxx.getVarAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vars = sorted(varsDict.items(), key=operator.itemgetter(1))\n",
    "sorted_vars.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write out to file\n",
    "with open('tmp/sorted_vars.txt', 'w') as f:\n",
    "    for item in sorted_vars:\n",
    "        f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all patterns that contain the words\n",
    "## this should be retrived from the table\n",
    "## e.g. game_reader[game_reader['pattern'].str.contains(\"tricare\")].sort_values('final_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vars[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = game_reader[game_reader['pattern'].str.contains(\"tricare\")].sort_values('final_score', ascending=False).head()\n",
    "ls['pattern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = game_reader[game_reader['pattern'].str.contains(r\"\\b\" + \"tricare\" + r\"\\b\", regex=True)]\n",
    "tmp.final_score.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "density = stats.gaussian_kde(tmp.final_score.values)\n",
    "ys = density(tmp.final_score.values)\n",
    "bb = np.argmax(ys * -1)\n",
    "aa = ys[bb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes = signal.find_peaks(ys * -1)\n",
    "valley_indexes = valley_indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes = signal.argrelmin(ys)\n",
    "valley_indexes = valley_indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[valley_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get patterns with high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for entry in sorted_vars:\n",
    "    if entry[1]>0:\n",
    "        token = entry[0]\n",
    "        tmp = game_reader[game_reader['pattern'].str.contains(r\"\\b\" + token + r\"\\b\", regex=True)]\n",
    "        tmp_ls = tmp['pattern'].values.tolist()\n",
    "        tmp_ls.append(str(entry[1]))\n",
    "        output[token] = tmp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more specific version\n",
    "output = {}\n",
    "for entry in sorted_vars[:300]:\n",
    "    if entry[1]>0:\n",
    "        token = entry[0]\n",
    "        tmp = game_reader[game_reader['pattern'].str.contains(r\"\\b\" + token + r\"\\b\", regex=True)]\n",
    "        tmp_ls = []\n",
    "        for index, row in tmp.iterrows():\n",
    "            tmp_ls.append(row['pattern'] + ' ' + str(row['final_score']))\n",
    "        tmp_ls.append(str(entry[1]))\n",
    "        output[token] = tmp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write the output to file\n",
    "file1 = open(\"tmp/522highvarall403.txt\",\"w\") \n",
    "file1.write(json.dumps(output, indent=2, sort_keys=False))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get out of sample patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## e.g.\n",
    "tmp = game_reader[game_reader['pattern'].str.contains(r\"\\b\" + \"president\" + r\"\\b\", regex=True)]\n",
    "[avg, low, high] =mean_confidence_interval(tmp.final_score.tolist())\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg') # Bypass the need to install Tkinter GUI framework\n",
    " \n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.arange(start = 0, stop = 25, step = 1, dtype='int')\n",
    "data_y = np.random.random(25)*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes = signal.argrelextrema(data_y, np.less)\n",
    "valley_indexes = valley_indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(tmp.final_score.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [t * 10 for t in data]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kmeans object\n",
    "kmeans = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit kmeans object to data\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes = signal.find_peaks_cwt(data * -1, widths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[valley_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes = signal.argrelextrema(data, np.less)\n",
    "valley_indexes = valley_indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valley_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots()\n",
    "\n",
    "valley_x = valley_indexes\n",
    "valley_y = tmp.values[valley_indexes]\n",
    "ax.plot(valley_x, valley_y, marker='o', linestyle='dashed', color='red', label=\"Valleys\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate through sorted_Values\n",
    "## count > 30 + out of range\n",
    "\n",
    "output = {}\n",
    "for entry in sorted_vars[:30]:\n",
    "    token = entry[0]\n",
    "    tmp = game_reader[game_reader['pattern'].str.contains(r\"\\b\" + token + r\"\\b\", regex=True)]\n",
    "    [avg, low, high] = mean_confidence_interval(tmp.final_score.tolist(), confidence=0.99)\n",
    "    \n",
    "    tmp_ls = []\n",
    "    for index, row in tmp.iterrows():\n",
    "        if float(row['final_score']) < low or float(row['final_score']) > high:\n",
    "            tmp_ls.append(row['pattern'] + ' ' + str(row['final_score']))\n",
    "            print(low, \" \", row['final_score'], \" \", token, \" \", high)\n",
    "        else:\n",
    "            print(\"skip\")\n",
    "    \n",
    "    if len(tmp_ls) > 0:   \n",
    "        output[token] = tmp_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}