{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.insert(0, '/Users/beidan/RASHIP/nrde/autoautophrase/')\n",
    "from activelearning.featureExtraction.hierclusteringscipy import cleandata, normalize\n",
    "import tools.fileHandler as fh\n",
    "# import nbimporter\n",
    "# from activelearning.learning import interface\n",
    "# dh = interface.datahandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datahandler:\n",
    "    def __init__(self):\n",
    "        self.dataset = pd.read_csv('../../outputs/features_results_all_withphrase.csv')\n",
    "        self.dataset = self.cleandata(self.dataset, 17)\n",
    "        self.X_normalized = normalize(self.dataset.iloc[:, list(range(1,18))])\n",
    "        self.phrases = self.dataset.iloc[:, 0].values.tolist()\n",
    "        self.status = [0] * len(self.X_normalized)\n",
    "\n",
    "    def cleandata(self, dataset, columnidx):\n",
    "        f_knownphrase = dataset.iloc[:, [columnidx]].values\n",
    "        for i in range(len(f_knownphrase)):\n",
    "            if math.isnan(f_knownphrase[i]):\n",
    "                f_knownphrase[i] = 0\n",
    "        dataset.iloc[:, [columnidx]] = f_knownphrase\n",
    "        return dataset\n",
    "\n",
    "    def scoreItem(self, phrase, positive=False):\n",
    "        idx = self.phrases.index(phrase)\n",
    "        if positive:\n",
    "           self.status[idx] = 1\n",
    "        else:\n",
    "           self.status[idx] = -1\n",
    "    \n",
    "    def modifyItems(self, phraselist, positive=False):\n",
    "        for phrase in phraselist:\n",
    "            if positive:\n",
    "                continue\n",
    "            else:\n",
    "                self.scoreItem(phrase)\n",
    "    \n",
    "    def getAvailableItems(self):\n",
    "        return [t for idx, t in enumerate(self.phrases) if int(self.status[idx]) == 0]\n",
    "    \n",
    "    def getdata(self):\n",
    "        return self.dataset\n",
    "    \n",
    "    def getallidx(self, phrases):\n",
    "        return [self.phrases.index(t) for t in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = datahandler()\n",
    "dataset = dh.getdata()\n",
    "avail = dh.getAvailableItems()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "68 round in total.\n"
    }
   ],
   "source": [
    "group3 = fh.getwords('../../outputs/features_group3_part_selected.txt', split=False)\n",
    "group3 = [t for t in group3 if len(t.split(' '))>1]\n",
    "round = math.ceil(len(group3) / 20)\n",
    "print(\"{} round in total.\".format(round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              freq        f_len  max_subgram_freq  partial_order  min_support  \\\ncount  1345.000000  1345.000000       1345.000000    1345.000000  1345.000000   \nmean      6.198513     2.188848          1.176952       0.005948     0.296446   \nstd      29.692296     0.472481          7.824046       0.076922     0.867276   \nmin       1.000000     2.000000          0.000000       0.000000     0.000000   \n25%       1.000000     2.000000          0.000000       0.000000     0.070000   \n\n       mutual_confidence  f_avg_wordlength     f_pos_nn   f_pos_prop  \\\ncount             1345.0       1345.000000  1345.000000  1345.000000   \nmean                -1.0          7.726223     0.769283     0.000372   \nstd                  0.0          1.884635     0.245190     0.009637   \nmin                 -1.0          1.500000     0.333000     0.000000   \n25%                 -1.0          6.500000     0.500000     0.000000   \n\n        f_pos_verb    f_pos_adj  f_pos_deter  f_pos_empty  f_pos_other  \\\ncount  1345.000000  1345.000000  1345.000000       1345.0       1345.0   \nmean      0.050987     0.179172     0.000186          0.0          0.0   \nstd       0.148413     0.237488     0.006817          0.0          0.0   \nmin       0.000000     0.000000     0.000000          0.0          0.0   \n25%       0.000000     0.000000     0.000000          0.0          0.0   \n\n       f_completeness  f_postagUniqueness  is_know_phrase  \ncount          1345.0         1345.000000          1345.0  \nmean              0.0            0.079554             4.0  \nstd               0.0            0.270702             0.0  \nmin               0.0            0.000000             4.0  \n25%               0.0            0.000000             4.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>freq</th>\n      <th>f_len</th>\n      <th>max_subgram_freq</th>\n      <th>partial_order</th>\n      <th>min_support</th>\n      <th>mutual_confidence</th>\n      <th>f_avg_wordlength</th>\n      <th>f_pos_nn</th>\n      <th>f_pos_prop</th>\n      <th>f_pos_verb</th>\n      <th>f_pos_adj</th>\n      <th>f_pos_deter</th>\n      <th>f_pos_empty</th>\n      <th>f_pos_other</th>\n      <th>f_completeness</th>\n      <th>f_postagUniqueness</th>\n      <th>is_know_phrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.0</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.000000</td>\n      <td>1345.0</td>\n      <td>1345.0</td>\n      <td>1345.0</td>\n      <td>1345.000000</td>\n      <td>1345.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>6.198513</td>\n      <td>2.188848</td>\n      <td>1.176952</td>\n      <td>0.005948</td>\n      <td>0.296446</td>\n      <td>-1.0</td>\n      <td>7.726223</td>\n      <td>0.769283</td>\n      <td>0.000372</td>\n      <td>0.050987</td>\n      <td>0.179172</td>\n      <td>0.000186</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.079554</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>29.692296</td>\n      <td>0.472481</td>\n      <td>7.824046</td>\n      <td>0.076922</td>\n      <td>0.867276</td>\n      <td>0.0</td>\n      <td>1.884635</td>\n      <td>0.245190</td>\n      <td>0.009637</td>\n      <td>0.148413</td>\n      <td>0.237488</td>\n      <td>0.006817</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.270702</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.0</td>\n      <td>1.500000</td>\n      <td>0.333000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.070000</td>\n      <td>-1.0</td>\n      <td>6.500000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "## get items for training\n",
    "features_idx = dh.getallidx(group3)\n",
    "features = (dh.dataset.iloc[features_idx, :])\n",
    "feature_name_list = list(features.columns)\n",
    "X = features.values\n",
    "labels = np.ones((len(X),1))\n",
    "f_normalized = dh.X_normalized.iloc[features_idx, :]\n",
    "features.describe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1.])"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# test = fh.getwords('../../outputs/features_group4_part.txt', split=False)\n",
    "# test_idx = dh.getallidx(test)\n",
    "# test_normalized = dh.X_normalized.iloc[test_idx, :]\n",
    "# test_pred = rf.predict(test_normalized)\n",
    "# test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(f_normalized, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Features Shape: (1076, 17)\nTraining Labels Shape: (1076, 1)\nTesting Features Shape: (269, 17)\nTesting Labels Shape: (269, 1)\n"
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n                      max_features='auto', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=1000,\n                      n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                      warm_start=False)"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mean Absolute Error: 0.0 degrees.\n"
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', (np.mean(errors)), 'degrees.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why positive only?\n",
    "test = fh.getwords('../../outputs/features_group4_part.txt', split=False)\n",
    "# test_idx = dh.getallidx(test)\n",
    "# test_normalized = dh.X_normalized.iloc[test_idx, :]\n",
    "# test_pred = rf.predict(test_normalized)\n",
    "# test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface saved for activelearning\n",
    "for i in range(round):\n",
    "    if i < 2:\n",
    "        items = dh.phrases[i*20 : (i+1)*20]\n",
    "        items_tousers = [str(idx) + \"---\" + str(t) for idx, t in enumerate(items)]\n",
    "        print(items_tousers)\n",
    "        print(\"        ============       \")\n",
    "        choices = input()\n",
    "        clist = choices.split(',')\n",
    "        plist = [items[int(t)] for t in clist]\n",
    "        dh.modifyItems(plist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bit7e24a33c941b4710a8754b9c18004557",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}